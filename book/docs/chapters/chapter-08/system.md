System

System architecture for humanoid locomotion organizes perception, state estimation, planning, control, and actuation into layered modules with well‑defined interfaces. The lowest layer is hardware drivers that read IMUs, joint encoders, motor currents, and foot force/torque sensors (or FSR arrays), and expose low‑latency state estimates via a real‑time transport (e.g., ROS 2 RTPS over DDS with real‑time kernels). A state estimator fuses these measurements to compute CoM position, base pose, joint velocities, and contact states. Common estimators include EKFs that operate on IMU and kinematics, and factor‑graph optimizers (GTSAM) for offline refinement.

The mid‑level planner produces footsteps and timing from objectives and constraints: discrete footstep planners (A*, RRT variants on foothold graphs), continuous trajectory optimizers, or receding‑horizon Model Predictive Control (MPC) that outputs CoM trajectories and contact forces. Planners must enforce kinematic reachability, ZMP/Capture Point margins, and friction constraints. The planner exposes an API: Inputs — desired velocity/heading, terrain model, max step size/height; Outputs — timed footstep sequence, CoM reference trajectory, foot swing trajectories. Planning modules should provide rich diagnostics (reachability maps, margin visualizations) to support tuning and debug.

The low‑level controller translates references to actuator commands. Options: inverse dynamics QP controllers (whole‑body QP with torque limits and contact friction cones), task‑space impedance controllers, or joint‑space PID cascades for position‑controlled robots. Real robots need safety envelopes (joint limits, torque limits, workspace boundaries) and monitoring (watchdog timers, state machine fallbacks) that gracefully switch to protective behaviors (e.g., stiffen joints, reduce speed, or execute sit‑down sequences). Deterministic scheduling (fixed‑rate control loops, high‑resolution timers) and time synchronization (PTP/NTP) are critical to avoid subtle phase‑errors between estimator and controller.

Practical deployment details matter: calibrate IMU biases, encoder zero positions, and foot sensor offsets with motion capture (VICON) or force plates; verify TF tree consistency using tf2 tools; and measure end‑to‑end latency from sensor read to actuator command. Middleware vendor choices (eProsima Fast‑DDS, RTI Connext) and kernel configuration (PREEMPT_RT) influence jitter and message latency — these are operational knobs with measurable impact. A physically grounded example: during treadmill walking trials a 1.4 m humanoid showed a reduction in CoM tracking RMS error and fewer MPC constraint violations after switching to a PREEMPT_RT kernel and tuning DDS QoS for reliability over latency. In that trial the diagnostics included recorded /joint_states, /imu, and /foot_forces topics, and the improvement was visible in reduced estimator residuals and fewer watchdog triggers.

Interface stability: lock message schemas (ROS 2 interface versions), version robot description files (URDF/SDF), and provide adapter layers when internal representations change. Document runtime configuration (QoS, control frequency, torque limits) and provide reproducible launch descriptions so simulation and hardware use the same contracts.