System

A practical digital‑twin system for robotics is a layered collection of services and well‑defined contracts that separate concerns: a physics fidelity layer, a sensor abstraction layer, a middleware layer, a rendering layer, and an orchestration/recording layer. The physics fidelity layer (Gazebo or NVIDIA Isaac) is responsible for numerically integrating rigid‑body dynamics, contact generation, joint limits, and actuator models; it exposes deterministic seeds, timestep and solver parameters, and actuator bandwidth knobs. The sensor abstraction layer implements geometric and stochastic models for lidar, depth and RGB cameras, IMUs, and tactile sensors; these models must expose configurable noise, latency, sampling rate, and per‑ray/pixel properties so they can be calibrated against empirical traces. The middleware layer (ROS 2 over DDS) provides topic and service contracts, TF frames, and time synchronization primitives (simulated ROS 2 clock) so the same nodes are runnable against sim or hardware. The rendering layer (Unity or Isaac Sim with RTX) is invoked selectively when photorealism or complex lighting is required; rendering runs are decoupled from the physics timestep when possible and synchronized via frame‑timestamps and TF lookups. The orchestration layer provides container images, launch descriptions, bridge nodes (ros_gz_bridge, ROS‑TCP‑Connector), rosbag2 storage, and CI job definitions so experiments are reproducible and automatable.

Designing the system requires explicit contracts: which topics carry ground truth, which are noisy sensor feeds, and which controllers may be swapped between sim and hardware. Provide a small set of canonical message namespaces and TF frame conventions to avoid drift between renderers and physics engines. Instrumentation is critical — expose telemetry topics for solver iterations, contact events, and bridge latencies so parity metrics can be computed post‑run.

Physically grounded example: build a twin where Gazebo (using SDF) simulates a mobile manipulator’s dynamics and contact interactions, publishing /joint_states, /tf and a simulated depth camera topic. Deploy ros_gz_bridge to translate Gazebo messages to ROS 2 topics. Run a separate Unity process (Unity Robotics Hub / ROS‑TCP‑Connector) that subscribes to the simulated camera’s TF and requests RGB renders at specific timestamps. Synchronize the system using ROS 2 simulated clock for deterministic replay and host NTP/Chrony to bound hardware time drift on HIL benches. Record experiments as rosbag2 archives and a small JSON manifest that lists URDF/SDF versions, plugin parameters, and random seeds.

Trade‑offs and invariants: favor composability over monolithic fidelity. Use Gazebo/Isaac for contact and control fidelity and Unity/Isaac RTX purely for photorealistic imagery when needed. Always version the twin artifacts (URDF/SDF, Xacro, plugin parameters) and treat the twin as an instrument subject to continuous calibration and automated parity checks.